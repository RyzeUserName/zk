# 什么是一致性？

​	1.火车站售票，车票信息（强一致性）

​	2.银行转账，延时转账（弱一致性《会话一致性，用户一致性》）

​	3.网上购物（最终一致性）

# 产生的原因？

​	分布式的发展，带来的副作用，数据的备份带来的数据的一致性的挑战

# 1.分布式架构

​	分布式系统是一个硬件或软件组件分布在不同的网络计算机上，彼此之间仅仅通过消息传递进行通信和协调的系统

​	特点：

​		1.分布性

​		2.对等性

​		3.并发性

​		4.缺乏全局时钟

​		5.故障总会发生

​	带来的问题：

​		通信异常

​		网络分区

​		三态

​		节点故障

# ACID（事务）

​	原子性 Atomicity

​	一致性 Consistency

​	隔离性 Isolation

​	持久性 Durability

数据库隔离级别 Read Uncommitted  

​			  Read commmited

​			  Repeatable read

​			 Serializable				  	

# 分步式事务

​	指事务的参与者、支持事务的服务器、资源服务器以及事务管理器分别位于分布式系统的不同节点之上

# CAP

​	一个分布式系统不可能同时满足一致性、可用性、和分区容错性，最多只能同时满足其中的两项

​	一致性：数据在多个副本下保持一致

​	可用性：有限的时间返回结果

​	分区容错性：对外提供一致性可用性的服务

# BASE

​	最终一致性  经过一段时间后，最终能够达到一个一致的状态

​	基本可用  时间和功能上的缺失

​	软状态  数据副本存在延时

最终一致性：1.A到B，B可以读到A的信息，而C不行 （因果一致性）

​		      2.A更新完，只能看到A的信息 （读己之所写）

​		      3.一个会话里，可见性 （会话一致性）

​		      4.一个系统读到新值后，之后都是新值（单调读一致性）

​		      5.一个系统保证同一进程的写操作被顺序执行	

# 2.一致性协议

##  2pc	

​	第一阶段：事务咨询->执行事务->询问结果

​	第二阶段： 发送提交请求->事务提交（回滚）->反馈执行结果->完成（中断）事务

​	优点：原理简单，实现方便

​	缺点：同步阻塞（参与者），单点问题（协调者），脑裂（数据不一致），太过保守（容错性无，仅靠超时）

## 3pc	

​	CanCommit 事务询问 -> 各参与者向协调者反馈事务咨询的响应

​	PreCommit  发送提交（中断）请求->事务预提交（中断）->各参与者向协调者反馈事务咨询的响应

​	doCommit  发送提交（中断）请求->事务提交（回滚）->反馈事务提交（回滚）结果->完成（中断）事务

​	优点：降低参与者的阻塞范围

​	缺点：第二阶段出现网络分区之后导致数据不一致

## PAXOS算法

​	1.提案选中

​		阶段1： 提案提出者提出编号为M的提案发送给半数以上的提案接收者，

​			      编号M大于提案接收者响应的所有提案的编号，那么返回历史处理的最大的编号N。

​		阶段2： 当半数以上的提案提出者发出M收到N的响应，那么就会发送一个【m,n】提案，

​			      只要提案接收者尚未对编号大于M的提案作出回应，那么就会通过这个提案

​	2.提取提案

​		方案1：每个提案接收者向提案执行者告知（次数m*n）

​		方案2：每个提案接收者向一个主的执行者汇报情况，主的在向其他扩散（次数m+n）

​		方案3：每个提案接收者向小与n的执行者汇报情况，再由其进行扩散（次数介意m+n与m*n之间）

# 3.paxos的实践

## 1.Chubby

​	-需要访问中心化节点的分布式锁服务（而不是一个一致性协议的客户端库）

​	应用场景：元数据存储，服务器的选举

​	锁服务：

​		锁延迟：解决网络原因导致的消息延迟

​		锁序列： 解决网络原因导致的消息乱序

​	事件通知机制：

​		客户端向服务端注册事件通知

​	缓存：

​		客户端缓存（通过续期的方式，与master的租期保持缓存一致性《强一致性》）

​	会话（客户端与服务端的tcp链接）---keep alive->会话维持（会话超时以客户端时钟为准+宽限期《默认45s》）

​	服务端的基础架构：

​		最底层---容错日志系统（强一致）

​		中间--- key-value类型的容错数据库

​		应用层---对外提供的分布式锁服务+小文件存储

​	快照：状态机的副本（用于其他机器/自己恢复）

​	事务日志：操作的提案

## 2.Hypertable

​	c++语言开发的高性能、可伸缩数据库（支持增删改查，不支持事务和关联查询）

​	支持海量数据管理和大量并发请求，扩展性好，可用性好

​	组成：hyperspace+RangeServer+Master+DFS Broker

hyperspace的算法实现：

​	1.hyperspace集群选取一个Active（剩下全是standby),master连接Active Server(也可对外提供服务)

​	2.BDB服务集群也会存在master角色，事务请求-->master->active->BDB master( 内部投票一半以上应用了该事务操作)	-->反馈Hypertable 的服务器更新成功

​	3.Active 宕机，剩下的进行选举（根据事务更新日志最新的《数据最全的机器容易被选举》），之后与之同步

# 4.zookeeper与paxos

## 1.基本概念

​	典型的分布式数据一致性解决方案，可用于数据发布/订阅、负载均衡、命名服务、分布式协调/通知、集群管理、master选举、分布式锁和分布式队列。

​	顺序一致性

​	原子性

​	单一视图性

​	可靠性

​	实时性

1.集群角色 Leader (提供客户端读写功能)Follower(提供读服务) Observer(提供读服务，但不参与选举也不参与过半写成功)。

2.会话session，一个有效的客户端的tcp长连接

3.数据节点Znode（数据模型中的数据单元）（分为持久节点/临时节点（与会话绑定的节点））《 机器节点指每台服务器》

4.版本 每个Znode存储的数据为 stat的数据结构,记录着Znode的三个数据版本 version（当前版本）、cversion（当前Znode的子节点版本）、aversion（当前znode的ACL版本）

5.watcher 允许用户在指定节点上注册一些watcher，并在特定事件发生时，服务端将时间通知到感兴趣的客户端上

6.ACL 权限控制 CREATE 创建子节点 READ 获取节点数据和子节点列表 WRITE 更新节点数据 DELETE 删除子节点   ADMIN 设置节点ACL 权限

## 2.zab 协议

​	是一种专门为zoopeer设计的崩溃可恢复的原子消息广播算法

​	主备模式的系统架构（以此保证集群中各副本之间的数据一致性）

​	所有的事务请求必须有一个全局唯一的服务器协调处理也就是（leader）接收并处理客户端的所有事务请求，并采用ZAB协议，广播事务Proposal(提议)到副本（Follower）服务器，leader等待反馈，当超过一半follower 正确反馈后，leader向follower发出commit的消息

​	1.消息广播模式

​		正常的leader-----(proposal)--->follower-----(ack 过半)--->leader---(commit)---->follower

​		为每个事务提供全局的单调递增的唯一ID ZXID 严格按照其排序处理

​	2.崩溃恢复模式

​		leader崩溃，选举产生新的leader，然后数据同步

​		需要选举集群中所有机器最高编号事务的为leader

​                ZXID64位，低32位为递增的编号，高32位为年代，每次leader更换的的周期epoch编号

​	进一步细分为三个阶段：

​		发现 ----------- 同步 -------------- 广播   循环执行，一个循环为一个主进程周期

​		1.发现：

​			选举阶段，选取leader

​			follower将自己最后接收到事务的年代值（epoch）发送给准leader

​			准leader接收到过半的消息后，将该值+1发送给follower

​			follower收，若年代值小于收到则替换，并发送给准leader ack包含年代，以及follower的历史 事务集和

​			leader 在集合中选出年代最大，事务ZXID最大的集合

​		2.同步

​			准leader 发送年代值和事务最多的集合 给这些follower 

​			若年代不同，则进行下轮循环，若相等，则接受事务集合反馈给leader

​			leader 接收到过半follower的反馈，发送commit给follower

​			follower执行commit，完成

​		3.广播（接收客户端消息）

​			

# 5.使用

​	1.部署运行

​	centos 6 + jdk 1.8 + zookeeper 3.4.10

​	单机模式、集群模式、伪集群模式

​	下载之后将 conf 目录下的 zoo_sample.cfg 修改为zoo.cfg

​	集体内容参考官网api

# 6.使用场景

​	1.数据发布/订阅

​		即所谓的配置中心，顾名思义，将数据发布到一个节点/一系列节点，供订阅者数据订阅，进而达到动态的配置	  

​	数据的目的，实现配置信息的集中式管理和数据动态更新。

​		存在推拉两种模式，推数据是具体的信息，拉数据需要定时轮询。zookeepe则是两种结合，客户端向服务端注

​	策需要关注的点，当发生变化时，服务端通知客户端发生了变化，客户端需要自行获取数据

​		通常我们可以 1.配置放在本地文件，服务运行时定时读取

​					2.JMX实现系统运行时内存变量的更新

​		但是，随着集群数量变多，频繁修改，以上变得很困难。

​	2.负载均衡

​		DNS服务为例，软负载

​		将需要解析的域名作为zookeeper的一个数据节点，维护的数据即为要的实际的映射地址

​		当然也可以实现自动化的域名解析管理sdk

​	3.命名服务

​		类似于资源映射之类，可以用zookeeper的创建子节点的时序性维护全局的资源唯一定位和映射

​	4.分布式协调/通知

​		分布式环境下，需要一个协调者，（分布式事务）不同客户端对同一个节点注册watcher，当数据变化，通知发

​		出，作出相应的处理

​		mysql的数据复制总线: 

​			热备：任务节点下    具体任务下   临时节点 两个或三个主机的 ip

​			冷备：任务节点下    具体任务下   临时节点 1个ip （但是 会持续进行，任务扫描，没有ip 分配ip）

​		用临时节点检测是主机否存在（故障）

​		将任务进度写在 任务节点上或者 进度节点上

​	5.集群管理

​		集群的监控+集群的控制

​		普通的是agent 实现

​		但是随着集群变大，存在升级困难，同一的agent不能满足多样性，编程语言的多样性

​		而zookeeper watcher+临时节点 可以满足集群的管理

​		分布式日志收集

​			存在的问题：1.变化的日志源机器

​						2.变化的日志收集器

​			收集器注册-> 在zookeeper 上注册收集器

​			根据日志源机器的列表分组然后 分配给  zookeeper具体的收集器的数据里

​			每个	收集器 定期写执行进度 到自己节点下的status 节点（以此判断 是否活着）（持久节点）

​			当有机器加入或者有机器宕掉，需要重新动态分配（日志系统的主动轮询）（监听太费资源）

​				1. 全局重新分配

​				2.根据每台机器负载，将新增的机器或者任务动态的分配

​		云主机的管理		

​	6.Master 选举

​		其实只要保证数据的唯一性（通过每台机器同时创建节点，成功创建的就是Master，其他的注册监听

​	（下次选举））

​	7.分布式锁

​		在实际开发中，我们很少在意分布式锁，仅仅利用关系型数据库固有的排他性来实现不同进程之间的互斥，这

​	确实是简单且被广泛使用的分布式锁的实现，但是随着分布式的扩展，性能的瓶颈在数据库操作上，如果再给数据库

​	加上锁（行锁，表锁甚至更繁重的任务）那么数据不堪重任

​		zookeeper的分布式锁：1.排他锁

​								X锁，写锁，独占锁。

​								向某个节点 注册临时节点，成功（只有一个）创建就认为获取了锁，其他的注册		

​							监听，等待锁释放，而锁释放有两种（1.正常完成，删除临时节点 2.获取锁的机	

​							器宕机，临时节点移除），只要自己不是序号最小的那么就没有获取锁

​							2.共享锁

​								S锁，读锁，共享锁。

​								向某个节点 注册临时序列节点，都会成功，当自己的序号最小，或者比自己序号

​							小的没有写操作，那么获取了锁，可以进行读取，但是如果，比自己序号小的有写操		

​							作，那么等待等待watcher通知，释放锁同上

​					改进读写操作顺序（因为羊群效应）：

​							读写操作不需要监听整个锁数据节点下的变更，因为大部分都是没用的，所以，写操

​						作，只需要监听比自己小的序号的事务，读操作需要监听的比自己小的读操作	​					

​	8.分布式队列

​		1.FIFO队列

​			在某个节点下，创建临时序列节点，向别自己小的序号注册监听，否则自己就是最小的，那么执行队列

​		2.等到队列元素聚合之后，在安排Barrier模型

​			在某个节点下，创建临时序列节点，并把需要聚合的数据写进该节点数据里，并注册监听，达到数量后执

​		行，否则等待通知			

​	9.hadoop

​		在hadoop中，zookeeper主要用于实现HA ，Hadoop Common的HA模块、HDFS的NameNode与YARN的

​	ResourceManager都是基于此模块 实现HA

​		YARN 的ResourceManager 的HA 基于此

​		ResourceManager 的主备切换 多台同时注册临时节点，只有一个成功，那么就是Master 其他注册监听。

​		Fencing（分离）分布式的 ”脑裂“ 	ResourceManager1 "假死"，此时	ResourceManager2成为Master，而

​	ResourceManager1 恢复之后，多个Active的主机，借助zookeeeper的ACL 权限，为了让Active 独占该节点，	

​	ResourceManager1 的信息都会被移除，ResourceManager2 Active之后重新创建并分配ACL 权限，那么

​	ResourceManager1  恢复了也没用，没有权限操作

​		ResourceManager 中的RMstateStore 需要存储一些关于Rm 运行中的信息，不需要持久化，官方建议使用

​	zookeeper存储，创建临时节点信息

​	10.HBase

​		HBase 在写入操作和索引上强一致性，使用zookeeper对整个系统做分布式协调

​		1.系统容错

​			RegionServer的感知存在，通知HMaster

​		2.RootRegion的管理

​			RootRegion的信息存储在zoopeeper上

​		3.Region状态管理

​			状态管理在zookeeper 上

​		4.分布式SplitLog 任务管理

​		5.Replication 管理（主备管理）

​	11.kafka

​		用于实现低延迟的发送和收集大量事件和日志数据

​		1.Broker的管理

​		2.Topic 注册后，Topic分区信息及Broker的对应关系

​		3.生产者负载均衡

​		4.消费者负载均衡

​		5.消息分区域消费者的关系

​		6.消息消费的offset

​		7.消费者到消费者组 

在阿里巴巴实际案例

1.消息中间组件Metamorpthosis

​	采用的是拉模型

​	1.生产者负载均衡  生产者获取zk 的broker 服务器以及分区信息，生产者通过轮询列表，周而复始发送，而zk负责

broker的连接有效性

​	2.消费者负载均衡（不同分组的负载均衡）	

​	3.消费位点Offset 存储 （消费位点存在zk 中）

2.Rpc 框架Dubbo

​	1.注册中心

​	2.集群容错

​	3.负载均衡

3.Mysql Binlog的增量订阅和消费组件：Canal

​	对假死server 可以用延时策略解决 

4.分布式数据库同步系统 Otter

​	SEDA 模型调度，类似于数据仓库的ETL 数据接入 数据提取 数据转换 数据载入

​	依赖zookeeper 的Observer 模式 观察者 对非事务请求处理能力

5.轻量级分布式通用搜索平台：终搜

​	centerNode 集群 用zookeeper为内核做的中心节点，完成元数据的保存和系统容错

​	在zookeeper的基础上进行二次开发，重写了选举的方法

​	配置文件动态管理也是zookeeper

​	全量任务执行也是靠zookeeper 的选举 master 主机执行

​	dns也是靠zookeeper 解析

​	zookeeper 管理 索引分区的情况

​	

6.实时计算引擎：JStorm

​	心跳的维护

​	同步任务配置

​	调度器选举

​	zookeeper优化

​		减少全量扫描

​		较少无用watcher

​		延长心跳

​		增加重试次数

# 7 .技术内幕

1.系统模型

​	数据模型

​		类似于unix的文件系统，最小节点ZNode，由/ 表示上下级关系，创建时为每个分配ZXID，全局64位唯一ID，每

​		次更新都会变更

​	节点特性

​		临时节点，持久节点，序列节点  节点的序列由父级维护

​	版本

​		保证原子性操作，乐观锁的实现

​	watcher

​		客户端注册监听到watcherManager，变更信息，客户端线程取回回调，执行到相应逻辑

​		监听自身节点变化，只要更改（更改内容一样也是会触发）就会触发

​		只能监听子节点数量变化，不能监听数据变化

​		权限AuthFailed 当使用错误的scheme 才会触发，权限值不对是不会触发

​		回调方法 watchedEvent 包括路径、事件类型、通知状态，最终会被包装成watcherEvent 用于传输给客户端

​		特点：一次性，需要反复注册

​			   客户端串行​					

​	ACL  scheme: id: permission

​		scheme: 1.IP    "ip:192.168.0.11"

​				2.Digest   "username:password"   会进行加密算法  SHA-1+BASE64

​				3.World 最开放的权限控制模式 "world:anyone"

​				4.Super 超级用户

​		授权对象ID

​		权限permission CREATE(C) DELETE(D) READ(R) WRITE(W) ANMIN(A)

2.序列化与协议jute

​	len  请求头 请求体                   len 响应头  响应体

3.客户端

zookeeper实例：客户端的入口

ClientWatchManager ： 客户端Watcher 管理器

HostProvide： 客户端地址列表管理

ClientCnxn: 客户端核心线程（sendThread   客户端发请求到服务器 EventThread 负责对服务端的事件处理   ）

# 8.RAFT算法

Raft是一种为了管理复制日志的一致性算法,相对于paxos更易于理解，主要分为领导选举、日志复制和安全三个模块

![](.\assets\状态机.jpg)

1.领导选举

​	心跳触发选举，领导者周期性的向所有跟随者发送心跳包，维护自己的权威，当一个跟随者在一段时间没有收到任何

消息，也就是选举超时，那么他会认为系统中没有可以用的领导者，并且发起选举已选出新的领导者

​	开始依次选举之前，跟随者增加当前任期号并转成候选人状态。选举中会向其他服务器节点，发送请求投票给自己，

候选人一直保持当前状态直到，有领导人选出或者没选出开始下一轮选举。

​	当获取到一个任期服务器节点一半以上投票（一个节点一个任期只有一票，先到先得原则），就会成为领导者，一旦

成为领导人，向其他节点发送心跳维持权威，并告知停止选举。

​	当收到的心跳任期不小于当前节点任期，当前节点成为跟随者，若收到心跳任期小于自己任期，那么拒绝此次，维持

现状。

​	当候选人既没有赢也没有输的情况，多个跟随者同时成为候选人，选票有可能被瓜分，以至于没得到大多数支持，

那么就会等选举超时，开始下一任期选举，使用的是随机选举超时时间，确保很少出现这种情况，这样服务器都分散开来

以至于大多数情况下只有一个服务器会选举超时，然后它赢得选举，同样的机制用在瓜分选票上，觉少了被瓜分的情况

![](.\assets\选举.jpg)

2.日志复制

选举出领导人之后，开始为客户端提供服务，客户端的每个请求都包含一条被复制状态及执行的指令。

领导人把这条指令当做一条新的日志条目附加到日志中去，然后并行的发起附加条目给其他的服务器，让他们复制这条日

志条目。当这条日志条目被安全的复制，领导人会应用这条日志条目到自己的状态机中，然后把执行的结果返回给客户

端。如果跟随者运行缓慢/崩溃/网络丢包，领导人会不断重复尝试附加日志条目（已经回复客户端），知道所有跟随者执

行。当大多数状态机执行了这个条目，日志条目会被提交。

